# Ferrum Infer (Rust) â€” vLLMâ€‘inspired Inference Core (Preâ€‘alpha)

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Ferrum Infer is a Rust research project exploring vLLMâ€‘style LLM inference. The repository currently focuses on clean abstractions (traits) and crate layout. There is no endâ€‘toâ€‘end runnable server yet.

## âš ï¸ Project Status

- **Preâ€‘alpha / Work in Progress**
- **Interfaces only**: most modules expose traits and skeletons; many implementations are placeholders
- **Not productionâ€‘ready**: APIs and code structure will change

## âœ¨ Goals (aligned with vLLM)

- **PagedAttention KV cache** with block tables and memory swapping
- **Continuous batching** (prefill/decode separation, tokenâ€‘level scheduling)
- **Prefix cache / prompt deduplication**
- **Backend abstraction** (Candle/ONNX Runtime; CUDA/ROCm/Metal/CPU)
- **OpenAIâ€‘compatible API** with streaming

## ğŸ“¦ Crate Overview

- `crates/ferrum-core`: core types and traits (`InferenceEngine`, `Backend`, `Scheduler`, `BatchManager`, `CacheManager`, `MemoryManager`, `Model`)
- `crates/ferrum-engine`: engine skeleton (executor, batch manager, paged KV cache, attention interfaces)
- `crates/ferrum-models`: model abstractions and config
- `crates/ferrum-cache`: cache traits and types
- `crates/ferrum-runtime`: runtime and device/memory abstractions
- `crates/ferrum-scheduler`: scheduling/batching configuration types
- `crates/ferrum-server`: OpenAIâ€‘compatible data types (HTTP server not implemented)
- `crates/ferrum-cli`: CLI scaffolding (TBD)

## âœ… What Exists Today

- Wellâ€‘scoped traits for engine, backend, scheduler, cache, memory, and models
- Skeletons for paged KV cache and attention with placeholder logic
- OpenAIâ€‘compatible request/response types under `ferrum-server`

## ğŸš« Not Implemented Yet

- Real backends (Candle/ORT) and model execution
- FlashAttention/PagedAttention GPU kernels and efficient memory management
- Prefill/Decode separation, preemption, prefix cache reuse
- OpenAI HTTP server, SSE streaming, auth, and metrics wiring
- Benchmarks and performance guarantees

## ğŸ”§ Build

Prerequisites: Rust 1.70+ (latest stable recommended)

```bash
git clone https://github.com/sizzlecar/ferrum-infer-rs.git
cd ferrum-infer-rs
cargo build
```

Note: Building succeeds for trait/skeleton crates; running a server is not supported yet.

## ğŸ—ºï¸ Roadmap (highâ€‘level)

1) MVP
- Plug in Candle backend, tokenizer, and basic sampling
- Singleâ€‘node dynamic batching, naive perâ€‘sequence KV cache
- OpenAI `/v1/chat/completions` with streaming (SSE)

2) vLLMâ€‘style features
- PagedAttention: block tables, allocation/eviction/compaction, GPU/CPU swap
- Prefill/Decode separation, tokenâ€‘level continuous batching, prefix cache reuse
- Scheduler with fairness/priority and bucketed padding

3) Performance and scale
- FlashAttention/fused kernels via FFI; quantization options
- Multiâ€‘GPU (tensor/pipeline parallel), improved observability and benchmarks

## ğŸ¤ Contributing

Contributions are welcome! The project is rapidly evolving; please open an issue to discuss design/implementation before large changes.

## ğŸ“ License

Licensed under the MIT License. See `LICENSE`.

## ğŸ™ Acknowledgements

Inspired by the design principles behind vLLM and Rust community projects (Candle, Axum, Tokio).