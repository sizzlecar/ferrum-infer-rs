[package]
name = "ferrum-infer"
version = "0.1.0"
edition = "2021"
authors = ["sizzlecar <sizzlecar@example.com>"]
description = "A high-performance Rust-based LLM inference engine MVP"
license = "MIT"
repository = "https://github.com/sizzlecar/ferrum-infer-rs"

[dependencies]
# Web framework
actix-web = "4.4"
actix-cors = "0.6"

# Async runtime
tokio = { version = "1.35", features = ["full"] }

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# Logging and monitoring
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter", "json"] }
tracing-actix-web = "0.7"

# Configuration
config = "0.14"

# Error handling
anyhow = "1.0"
thiserror = "1.0"

# Utilities
uuid = { version = "1.6", features = ["v4"] }
chrono = { version = "0.4", features = ["serde"] }
once_cell = "1.19"
async-trait = "0.1"
toml = "0.8"
async-stream = "0.3"
futures-util = "0.3"

# Performance monitoring
metrics = "0.22"
metrics-exporter-prometheus = "0.13"

# Memory management
parking_lot = "0.12"

# ML/DL framework (optional dependencies)
candle-core = { version = "0.3", optional = true }
candle-nn = { version = "0.3", optional = true }
candle-transformers = { version = "0.3", optional = true }
candle-datasets = { version = "0.3", optional = true }
hf-hub = { version = "0.3", optional = true }
tokenizers = { version = "0.15", optional = true }

[features]
default = []
ml = ["candle-core", "candle-nn", "candle-transformers", "candle-datasets", "hf-hub", "tokenizers"]

[dev-dependencies]
criterion = { version = "0.5", features = ["html_reports"] }
mockall = "0.12"
tokio-test = "0.4"

[[bin]]
name = "ferrum-infer"
path = "src/main.rs"

[[bench]]
name = "benchmarks"
harness = false
