---
description: Rust best practices for Ferrum LLM inference (Candle/ORT backends, engine, cache, scheduler)
globs: crates/**/*.rs, src/**/*.rs, benches/**/*.rs, examples/**/*.rs
---

# Rust + LLM Inference (Candle) Best Practices

## Project Structure
- Keep `ferrum-core` free of heavy deps; only types, errors, traits.
- Implementations live in leaf crates (engine/runtime/server); depend one-way on `ferrum-core`.
- Respect crate boundaries: no cross-layer leaks; avoid cyclic features.
- Use feature flags for optional backends: `cuda`, `ort`, `server`, `cli`.

## Code Style
- Edition 2021; format with `cargo fmt`; lint with `cargo clippy`.
- Avoid abbreviations; prefer descriptive names (Clean Code). Functions are verbs, values are nouns.
- Prefer early returns and guard clauses. Avoid deep nesting.
- Do not use `unwrap`/`expect` in library code; return `Result<T, crate::error::Error>`.

## Error Handling
- Public APIs: use `thiserror`-based domain error; avoid exposing `anyhow::Error`.
- Internal glue code may use `anyhow` but convert at boundaries.
- Include context (`with_context`) on fallible IO/model loads.

## Concurrency & Async
- Use `tokio` everywhere; never block the async runtime.
- Do not hold locks across `.await`. Split critical sections and clone data first.
- Prefer `parking_lot::{Mutex,RwLock}` for in-process state; annotate lock scopes.
- Use `tokio::sync::mpsc` for streams; expose `ReceiverStream` for SSE/stream APIs.

## Logging & Metrics
- Use `tracing` (`trace/debug/info/warn/error`) with structured fields: `request_id`, `batch_id`, `model_id`.
- Use `tracing::instrument(skip(allocation_heavy_args))` on hot functions.
- Emit `metrics` for requests, tokens, latency, cache hits, swaps, evictions.

## Candle Tensors & Memory
- Minimize data movement: avoid unnecessary `.to_device()`, `.to_dtype()`.
- Treat `Tensor::clone()` as shallow but still prefer borrowing/slicing (`narrow`, `chunk`) over materializing copies.
- Pre-allocate KV buffers; reuse allocations across decode steps.
- Keep dtypes consistent (FP16/BF16 on GPU, FP32 on CPU unless justified).
- Avoid host<->device ping-pong; batch transfers; prefer pinned host memory for FFI.

## KV Cache (MVP Guidance)
- Start with per-request contiguous KV (no paging). Define clear layout `[layers, heads, seq, head_dim]`.
- Validate bounds and shapes aggressively in debug builds.
- Introduce paged/block tables behind a feature flag later.

## Batching & Scheduling
- MVP: dynamic batching with `{max_batch_size, max_wait_ms}`; pad within batch.
- Separate prefill vs decode in design, even if implemented later.
- Never add requests into a running decode batch for MVP (simplifies invariants).

## Sampling
- Implement numerically stable softmax/logits processing.
- Support greedy, temperature, top-k, top-p, stop sequences; deterministic path for tests.
- Keep RNG seeds plumbed for reproducibility.

## Backend Abstraction
- `Backend` trait owns device/tensor creation and model load/execution.
- Candle backend must not leak Candle types through public `ferrum-core` APIs.
- Gate CUDA-specific paths behind `cfg(feature = "cuda")`.

## Unsafe & FFI
- Prefer safe Rust. If `unsafe` is required (e.g., custom CUDA kernels), isolate in small modules.
- Document safety invariants above every `unsafe` block.
- Add tests/fuzz where memory safety relies on protocol/shape invariants.

## Testing Strategy
- Unit tests: tensor shapes, KV indexing, sampling edge cases.
- Integration: CLI generate path, server SSE stream with tiny model.
- Avoid large models in CI. Use tiny variants and fixtures via `hf-hub` caching.
- Use `#[cfg(feature = "ci-small-model")]` to gate heavy tests.

## API & Server
- OpenAI-compatible types live in `ferrum-server`; keep handlers thin and defer to engine.
- SSE: send small JSON deltas per token; terminate with `[DONE]` sentinel.
- Validate inputs; enforce limits (max tokens, rate limiting optionally) before scheduling.

## Performance Hints
- Avoid per-token heap allocations in hot loops; reuse buffers.
- Precompute shapes/strides outside loops.
- Consider `#[inline]` for tiny hot-path helpers; measure before committing.
- Instrument and benchmark: latency percentiles, tokens/sec, batch utilization.

## Configuration
- Use strongly-typed configs with sane defaults; validate at startup.
- Make dangerous knobs (e.g., huge batch size) explicit and documented.

## Security & Privacy
- Do not log prompts or raw tokens at `info`. Use `debug` with redaction.
- Keep API keys and secrets out of logs; support header-based auth behind a flag.

## CI
- Run `cargo fmt --check`, `cargo check --workspace --all-targets`, advisory `clippy`.
- Skip heavy benchmarks in CI; keep a tiny end-to-end smoke test.
